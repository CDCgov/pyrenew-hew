---
title: "Scoring helpers for `pyrenew-hew`"
format: gfm
echo: true
output: true
warning: false
---

## Scoring helpers

The goal of this note is to demonstrate the model scoring helpers in `scoringutilhelpers`.

First, we load the code:

```{r}
#| output: false
library(scoringutils)
library(ggplot2)
source("R/exampleprediction.R")
source("R/exampletruthdata.R")
source("R/join_forecast_and_data.R")
source("R/score_forecasts.R")
```

## Example data

Now, we can generate some example data. 

The `exampleprediction` function generates log-normal "forecasts" across a user selected number of areas and dates with 1 and 2 week lookaheads. 
In order to make the example data match the likely use-case we also generate `.chain` and `.iteration` fields to match a tidybayes-type data structure.
The example data is serialised locally in `/assets`.

The `exampletruthdata` function generates "truthdata" in a similar way, and also serialises to `/assets`.

```{r}
#Example predictions
examplepreds <- exampleprediction(savedata = TRUE)
#Example truth data
exampledata <- exampletruthdata(savedata = TRUE)
```

Note that the `examplepreds` is serialised in `parquet` format and `exampledata` as `tsv` format.

## Joining forecasts and the truth data

Preparation for scoring the forecasts is done by: 

1. Ingesting forecasts from the `forecast_source` directory and truthdata from `truthdata_file`. The forecasts are treated as an Arrow dataset and loaded with `arrow::open_dataset`. The truthdata is assumed to be a `tsv` file.
2. The joining operation is determined by `join_key`. In this case the forecast is _to_ a date in `target_end_date` scored with data recorded at that `date`.

```{r}
forecast_source <- "scoringutilhelpers/assets/examplepredictions"
truthdata_file <- "scoringutilhelpers/assets/exampletruthdata.tsv"
scorable_data <- join_forecast_and_data(forecast_source, truthdata_file,
        join_key = join_by(area, target_end_date == date)) |>
        collect()
scorable_data |> print(n = 10)        
```

## Scoring prepared forecasts

Scoring the forecast is done by:

1. Setting the [forecast unit](https://epiforecasts.io/scoringutils/dev/index.html#the-forecast-unit). In this case, we want to forecast on the area, reference date (date of forecast submission), the target date of the data and model (although only one "model" is in this example).
2. Transforming the data and forecasts. The forecasts and data in this example are non-negative, and the default is to score on the log-transformed data. `score_forecasts` function splats arguments to `scoringutils::transform_forecasts` to modify this.
3. Scoring the transformed forecasts and data.
4. If more than one model is provided `score_forecasts` does pairwise comparisons.

```{r}
forecast_unit <- c("area", "reference_date", "target_end_date", "model")
observed <- "truthdata"
predicted <- "prediction"

scored_forecasts <- score_forecasts(scorable_data,
        forecast_unit = forecast_unit,
        observed = observed,
        predicted = predicted,
    )
scored_forecasts
```

## Summary scores

`scored_forecasts` is the level of forecast we (likely) want to operate at; that is that is likely to be better to score multiple scenarios and _then_ summarise rather than summarising multiple scenarios.

However, we should note that we can further summarise `scored_forecasts`.

```{r}
summ <- summarise_scores(scored_forecasts, by = c("model", "area"))
summ
```
